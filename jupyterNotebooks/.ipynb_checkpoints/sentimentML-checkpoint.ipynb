{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 62FC-BD04\n",
      "\n",
      " Directory of C:\\Users\\thinb\\Desktop\\article-evaluator\\jupyter notebooks\n",
      "\n",
      "20/10/2020  15:54    <DIR>          .\n",
      "20/10/2020  15:54    <DIR>          ..\n",
      "20/10/2020  15:33    <DIR>          .ipynb_checkpoints\n",
      "20/10/2020  15:54    <DIR>          data\n",
      "20/10/2020  15:54             1,887 Untitled.ipynb\n",
      "               1 File(s)          1,887 bytes\n",
      "               4 Dir(s)  662,899,347,456 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a3547f6b81ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m                                 \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                                 \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                                 \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m                                 follow_links=False):\n\u001b[0;32m     43\u001b[0m   \"\"\"Generates a `tf.data.Dataset` from text files in a directory.\n",
      "\u001b[1;31mNameError\u001b[0m: name 'validation' is not defined"
     ]
    }
   ],
   "source": [
    "# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Keras text dataset generation utilities.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.data.ops import dataset_ops\n",
    "from tensorflow.python.keras.preprocessing import dataset_utils\n",
    "from tensorflow.python.ops import io_ops\n",
    "from tensorflow.python.ops import string_ops\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "\n",
    "directory=r\"C:\\Users\\thinb\\Desktop\\article-evaluator\\jupyter notebooks\\data\\test\"\n",
    "\n",
    "\n",
    "@keras_export('keras.preprocessing.text_dataset_from_directory', v1=[])\n",
    "def text_dataset_from_directory(directory,\n",
    "                                labels='inferred',\n",
    "                                label_mode='binary',\n",
    "                                class_names=None,\n",
    "                                batch_size=32,\n",
    "                                max_length=None,\n",
    "                                shuffle=True,\n",
    "                                seed=None,\n",
    "                                validation_split=0.2,\n",
    "                                subset='validation',\n",
    "                                follow_links=False):\n",
    "  \"\"\"Generates a `tf.data.Dataset` from text files in a directory.\n",
    "\n",
    "  If your directory structure is:\n",
    "\n",
    "  ```\n",
    "  main_directory/\n",
    "  ...class_a/\n",
    "  ......a_text_1.txt\n",
    "  ......a_text_2.txt\n",
    "  ...class_b/\n",
    "  ......b_text_1.txt\n",
    "  ......b_text_2.txt\n",
    "  ```\n",
    "\n",
    "  Then calling `text_dataset_from_directory(main_directory, labels='inferred')`\n",
    "  will return a `tf.data.Dataset` that yields batches of texts from\n",
    "  the subdirectories `class_a` and `class_b`, together with labels\n",
    "  0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).\n",
    "\n",
    "  Only `.txt` files are supported at this time.\n",
    "\n",
    "  Arguments:\n",
    "    directory: Directory where the data is located.\n",
    "        If `labels` is \"inferred\", it should contain\n",
    "        subdirectories, each containing text files for a class.\n",
    "        Otherwise, the directory structure is ignored.\n",
    "    labels: Either \"inferred\"\n",
    "        (labels are generated from the directory structure),\n",
    "        or a list/tuple of integer labels of the same size as the number of\n",
    "        text files found in the directory. Labels should be sorted according\n",
    "        to the alphanumeric order of the text file paths\n",
    "        (obtained via `os.walk(directory)` in Python).\n",
    "    label_mode:\n",
    "        - 'int': means that the labels are encoded as integers\n",
    "            (e.g. for `sparse_categorical_crossentropy` loss).\n",
    "        - 'categorical' means that the labels are\n",
    "            encoded as a categorical vector\n",
    "            (e.g. for `categorical_crossentropy` loss).\n",
    "        - 'binary' means that the labels (there can be only 2)\n",
    "            are encoded as `float32` scalars with values 0 or 1\n",
    "            (e.g. for `binary_crossentropy`).\n",
    "        - None (no labels).\n",
    "    class_names: Only valid if \"labels\" is \"inferred\". This is the explict\n",
    "        list of class names (must match names of subdirectories). Used\n",
    "        to control the order of the classes\n",
    "        (otherwise alphanumerical order is used).\n",
    "    batch_size: Size of the batches of data. Default: 32.\n",
    "    max_length: Maximum size of a text string. Texts longer than this will\n",
    "      be truncated to `max_length`.\n",
    "    shuffle: Whether to shuffle the data. Default: True.\n",
    "        If set to False, sorts the data in alphanumeric order.\n",
    "    seed: Optional random seed for shuffling and transformations.\n",
    "    validation_split: Optional float between 0 and 1,\n",
    "        fraction of data to reserve for validation.\n",
    "    subset: One of \"training\" or \"validation\".\n",
    "        Only used if `validation_split` is set.\n",
    "    follow_links: Whether to visits subdirectories pointed to by symlinks.\n",
    "        Defaults to False.\n",
    "\n",
    "  Returns:\n",
    "    A `tf.data.Dataset` object.\n",
    "      - If `label_mode` is None, it yields `string` tensors of shape\n",
    "        `(batch_size,)`, containing the contents of a batch of text files.\n",
    "      - Otherwise, it yields a tuple `(texts, labels)`, where `texts`\n",
    "        has shape `(batch_size,)` and `labels` follows the format described\n",
    "        below.\n",
    "\n",
    "  Rules regarding labels format:\n",
    "    - if `label_mode` is `int`, the labels are an `int32` tensor of shape\n",
    "      `(batch_size,)`.\n",
    "    - if `label_mode` is `binary`, the labels are a `float32` tensor of\n",
    "      1s and 0s of shape `(batch_size, 1)`.\n",
    "    - if `label_mode` is `categorial`, the labels are a `float32` tensor\n",
    "      of shape `(batch_size, num_classes)`, representing a one-hot\n",
    "      encoding of the class index.\n",
    "  \"\"\"\n",
    "  if labels != 'inferred':\n",
    "    if not isinstance(labels, (list, tuple)):\n",
    "      raise ValueError(\n",
    "          '`labels` argument should be a list/tuple of integer labels, of '\n",
    "          'the same size as the number of text files in the target '\n",
    "          'directory. If you wish to infer the labels from the subdirectory '\n",
    "          'names in the target directory, pass `labels=\"inferred\"`. '\n",
    "          'If you wish to get a dataset that only contains text samples '\n",
    "          '(no labels), pass `labels=None`.')\n",
    "    if class_names:\n",
    "      raise ValueError('You can only pass `class_names` if the labels are '\n",
    "                       'inferred from the subdirectory names in the target '\n",
    "                       'directory (`labels=\"inferred\"`).')\n",
    "  if label_mode not in {'int', 'categorical', 'binary', None}:\n",
    "    raise ValueError(\n",
    "        '`label_mode` argument must be one of \"int\", \"categorical\", \"binary\", '\n",
    "        'or None. Received: %s' % (label_mode,))\n",
    "  dataset_utils.check_validation_split_arg(\n",
    "      validation_split, subset, shuffle, seed)\n",
    "\n",
    "  if seed is None:\n",
    "    seed = np.random.randint(1e6)\n",
    "  file_paths, labels, class_names = dataset_utils.index_directory(\n",
    "      directory,\n",
    "      labels,\n",
    "      formats=('.txt',),\n",
    "      class_names=class_names,\n",
    "      shuffle=shuffle,\n",
    "      seed=seed,\n",
    "      follow_links=follow_links)\n",
    "\n",
    "  if label_mode == 'binary' and len(class_names) != 2:\n",
    "    raise ValueError(\n",
    "        'When passing `label_mode=\"binary\", there must exactly 2 classes. '\n",
    "        'Found the following classes: %s' % (class_names,))\n",
    "\n",
    "  file_paths, labels = dataset_utils.get_training_or_validation_split(\n",
    "      file_paths, labels, validation_split, subset)\n",
    "\n",
    "  dataset = paths_and_labels_to_dataset(\n",
    "      file_paths=file_paths,\n",
    "      labels=labels,\n",
    "      label_mode=label_mode,\n",
    "      num_classes=len(class_names),\n",
    "      max_length=max_length)\n",
    "  if shuffle:\n",
    "    # Shuffle locally at each iteration\n",
    "    dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  # Users may need to reference `class_names`.\n",
    "  dataset.class_names = class_names\n",
    "  return dataset\n",
    "\n",
    "\n",
    "def paths_and_labels_to_dataset(file_paths,\n",
    "                                labels,\n",
    "                                label_mode,\n",
    "                                num_classes,\n",
    "                                max_length):\n",
    "  \"\"\"Constructs a dataset of text strings and labels.\"\"\"\n",
    "  path_ds = dataset_ops.Dataset.from_tensor_slices(file_paths)\n",
    "  string_ds = path_ds.map(\n",
    "      lambda x: path_to_string_content(x, max_length))\n",
    "  if label_mode:\n",
    "    label_ds = dataset_utils.labels_to_dataset(labels, label_mode, num_classes)\n",
    "    string_ds = dataset_ops.Dataset.zip((string_ds, label_ds))\n",
    "  return string_ds\n",
    "\n",
    "\n",
    "def path_to_string_content(path, max_length):\n",
    "  txt = io_ops.read_file(path)\n",
    "  if max_length is not None:\n",
    "    txt = string_ops.substr(txt, 0, max_length)\n",
    "  return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
